{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import string\n",
    "import re\n",
    "from  sklearn.feature_extraction.text import TfidfVectorizer\n",
    "np.random.seed(69420)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_link = \"https://drive.google.com/file/d/199uKHh1aPSl46XVX_gCVgXasovlOsZ3T/view?usp=sharing\"\n",
    "test_link = \"https://drive.google.com/file/d/1LWR1cxvTM-BKkYyE0Gb0XrXRS59BPoco/view?usp=sharing\"\n",
    "\n",
    "train_id= train_link.split('/')[-2]\n",
    "test_id = test_link.split('/')[-2]\n",
    "\n",
    "#Prefix url\n",
    "start_url ='https://drive.google.com/uc?id=' \n",
    "\n",
    "#The dataframe\n",
    "df_train = pd.read_csv(start_url + train_id ,encoding=\"latin-1\")\n",
    "df_test = pd.read_csv(start_url + test_id, encoding=\"latin-1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokeinizing and removing the stopwords, punctuations and Special symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>modified_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "      <td>geosolutions technology leverage benefon gps s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "      <td>esi lows bk real possibility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>last quarter componenta net sales doubled peri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>according chamber commerce major construction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>swedish buyout firm sold remaining percent sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4860</th>\n",
       "      <td>3-star analyst Joe Wittine from Longbow Resear...</td>\n",
       "      <td>positive</td>\n",
       "      <td>analyst joe wittine longbow research reiterate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>Our standardised services have met with a posi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>standardised services met positive reception a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4862</th>\n",
       "      <td>The Kyroskoski investment is to be completed i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>kyroskoski investment completed late investmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4863</th>\n",
       "      <td>The Group 's cash flow from operations will be...</td>\n",
       "      <td>positive</td>\n",
       "      <td>group cash flow operations positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4864</th>\n",
       "      <td>7 March 2011 - Finnish IT company Digia Oyj HE...</td>\n",
       "      <td>positive</td>\n",
       "      <td>march finnish company digia oyj hel announced ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4865 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence Sentiment  \\\n",
       "0     The GeoSolutions technology will leverage Bene...  positive   \n",
       "1     $ESI on lows, down $1.50 to $2.50 BK a real po...  negative   \n",
       "2     For the last quarter of 2010 , Componenta 's n...  positive   \n",
       "3     According to the Finnish-Russian Chamber of Co...   neutral   \n",
       "4     The Swedish buyout firm has sold its remaining...   neutral   \n",
       "...                                                 ...       ...   \n",
       "4860  3-star analyst Joe Wittine from Longbow Resear...  positive   \n",
       "4861  Our standardised services have met with a posi...  positive   \n",
       "4862  The Kyroskoski investment is to be completed i...   neutral   \n",
       "4863  The Group 's cash flow from operations will be...  positive   \n",
       "4864  7 March 2011 - Finnish IT company Digia Oyj HE...  positive   \n",
       "\n",
       "                                     modified_sentences  \n",
       "0     geosolutions technology leverage benefon gps s...  \n",
       "1                          esi lows bk real possibility  \n",
       "2     last quarter componenta net sales doubled peri...  \n",
       "3     according chamber commerce major construction ...  \n",
       "4     swedish buyout firm sold remaining percent sta...  \n",
       "...                                                 ...  \n",
       "4860  analyst joe wittine longbow research reiterate...  \n",
       "4861  standardised services met positive reception a...  \n",
       "4862  kyroskoski investment completed late investmen...  \n",
       "4863                group cash flow operations positive  \n",
       "4864  march finnish company digia oyj hel announced ...  \n",
       "\n",
       "[4865 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Getting a list of english stopwords\n",
    "stopwords = [a.lower() for a in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "def process_sentence(str):\n",
    "    words = word_tokenize(str)\n",
    "    str = [word.lower() for word in words if word.isalpha()]\n",
    "    words = [re.sub(r'\\S+@\\S+', ' ', word) for word in str] \n",
    "    words = [word for word in words if word!=' ']\n",
    "\n",
    "    # Removing the words that are present in the stopwords\n",
    "    words=[word.lower() for word in words if word.lower() not in stopwords and word.strip() != \"\"]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df_train['modified_sentences'] = df_train.Sentence.apply(process_sentence)\n",
    "df_test['modified_sentences'] = df_test.Sentence.apply(process_sentence)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(str):\n",
    "    words = [lemmatizer.lemmatize(word) for word in str.split()]\n",
    "    return ' '.join(words)\n",
    "    \n",
    "df_train['modified_sentences'] = df_train.modified_sentences.apply(lemmatize_words)\n",
    "df_test['modified_sentences'] = df_test.modified_sentences.apply(lemmatize_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorising the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_train.modified_sentences\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectorizer.fit(sentences)\n",
    "vector = vectorizer.transform(sentences)\n",
    "X =(vector.toarray())\n",
    "y = df_train.Sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL AND PREDICTING THE SENTIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06167401 0.78129713 0.52840909]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "predictions= model.predict(X_test)\n",
    "\n",
    "## Calculating the f1_score\n",
    "print(f1_score(y_test,predictions,average=None))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the csv file for the predictions obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(str):\n",
    "    a=vectorizer.transform(str)\n",
    "    return (model.predict(a))\n",
    "\n",
    "predictions_actual_test = predict(df_test.modified_sentences)\n",
    "\n",
    "df_pred= pd.DataFrame(data={\"predictions\":predictions_actual_test})\n",
    "df_pred.to_csv('Predictions_sentiments.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
